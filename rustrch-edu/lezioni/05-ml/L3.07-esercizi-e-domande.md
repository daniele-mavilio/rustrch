# Esercizi e Domande

## Teoria

In questa sezione, metteremo alla prova la tua comprensione dei concetti chiave trattati nella fase di embedding. Rispondere a queste domande e completare gli esercizi ti aiuterà a consolidare quanto appreso e a prepararti per le prossime fasi del corso.

## Esempio

Ecco un esempio di come potresti rispondere a una domanda di comprensione:

**Domanda:** Cos'è un embedding e come viene utilizzato nella ricerca semantica?

**Risposta:** Un embedding è una rappresentazione vettoriale di un testo che cattura il suo significato semantico. Viene utilizzato nella ricerca semantica per trovare documenti che sono semanticamente simili alla query dell'utente, anche se non contengono le stesse parole chiave.

## Pseudocodice

Ecco uno pseudocodice che illustra come potresti implementare una funzione per calcolare la similarità tra embedding:

```rust
// Funzione per calcolare la similarità del coseno tra due vettori
funzione similarita_coseno(a: &Vec<f32>, b: &Vec<f32>) -> f32 {
    // Calcola il prodotto scalare tra i due vettori
    let prodotto_scalare = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum::<f32>();
    
    // Calcola la norma di ciascun vettore
    let norma_a = (a.iter().map(|x| x * x).sum::<f32>()).sqrt();
    let norma_b = (b.iter().map(|x| x * x).sum::<f32>()).sqrt();
    
    // Calcola la similarità del coseno
    prodotto_scalare / (norma_a * norma_b)
}

// Funzione per trovare documenti simili utilizzando la similarità del coseno
funzione trova_simili(query_embedding: &Vec<f32>, documenti: &[Documento]) -> Vec<(Documento, f32)> {
    documenti.iter()
        .map(|doc| {
            let similarita = similarita_coseno(query_embedding, &doc.embedding);
            (doc.clone(), similarita)
        })
        .filter(|(_, similarita)| *similarita > 0.5) // Filtra risultati con similarità > 0.5
        .sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap()) // Ordina per similarità decrescente
        .collect()
}
```

## Risorse

Ecco alcune risorse utili per approfondire gli argomenti trattati in questa fase:

1. [ONNX Runtime Documentation](https://onnxruntime.ai/)
2. [Sentence Transformers](https://www.sbert.net/)
3. [Understanding Embeddings](https://jalammar.github.io/illustrated-word2vec/)
4. [Cosine Similarity Explained](https://en.wikipedia.org/wiki/Cosine_similarity)

## Esercizio

### Domande di Comprensione

1. **Cos'è un embedding e come rappresenta il significato semantico del testo?**
2. **Qual è la differenza tra un motore di ricerca basato su keyword e uno basato su embedding?**
3. **Come funziona la similarità del coseno e perché è utile per confrontare embedding?**
4. **Qual è il processo per generare un embedding da un testo?**
5. **Come possiamo utilizzare un modello di machine learning per generare embedding in Rust?**

### Esercizi Pratici

1. **Calcolo della Similarità del Coseno:** Scrivi una funzione in Rust che calcoli la similarità del coseno tra due vettori di float. Testala con i seguenti vettori:
   - Vettore A: [0.2, 0.8, 0.1, 0.3]
   - Vettore B: [0.3, 0.7, 0.2, 0.4]
   - Risultato atteso: circa 0.974

2. **Generazione di Embedding:** Scrivi una funzione in Rust che simuli la generazione di embedding per una serie di documenti. Utilizza vettori casuali per rappresentare gli embedding e salvali in una struttura dati.

### Traccia di Soluzione

1. **Cos'è un embedding e come rappresenta il significato semantico del testo?**
   - Un embedding è un vettore di numeri che rappresenta il significato semantico di un testo. Ogni dimensione del vettore cattura un aspetto del significato, permettendo di confrontare testi in base alla loro similarità semantica.

2. **Qual è la differenza tra un motore di ricerca basato su keyword e uno basato su embedding?**
   - Un motore di ricerca basato su keyword trova documenti che contengono esattamente le parole chiave della query, mentre un motore basato su embedding trova documenti che sono semanticamente simili alla query, anche se non contengono le stesse parole chiave.

3. **Come funziona la similarità del coseno e perché è utile per confrontare embedding?**
   - La similarità del coseno misura l'angolo tra due vettori. Un valore di 1 indica che i vettori sono identici, mentre un valore di 0 indica che sono ortogonali. È utile per confrontare embedding perché misura quanto sono simili semanticamente.

4. **Qual è il processo per generare un embedding da un testo?**
   - Il processo per generare un embedding da un testo coinvolge la tokenizzazione del testo, la preparazione dell'input per il modello, l'esecuzione dell'inferenza e l'estrazione dell'embedding dall'output.

5. **Come possiamo utilizzare un modello di machine learning per generare embedding in Rust?**
   - Possiamo utilizzare un modello di machine learning per generare embedding in Rust caricando il modello, tokenizzando il testo, preparando l'input per il modello, eseguendo l'inferenza e estraendo l'embedding dall'output.
