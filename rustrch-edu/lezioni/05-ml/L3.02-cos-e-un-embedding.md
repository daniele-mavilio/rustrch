# Cos'è un Embedding

## Teoria

Un embedding è una rappresentazione vettoriale di un oggetto, come una parola, una frase o un documento, in uno spazio vettoriale continuo. Nel contesto della ricerca semantica, gli embedding sono utilizzati per catturare il significato semantico del testo, permettendo di confrontare e trovare documenti simili in base al loro contenuto semantico piuttosto che alle parole chiave.

Gli embedding sono generati utilizzando modelli di machine learning addestrati su grandi quantità di testo. Questi modelli imparano a mappare il testo in uno spazio vettoriale in cui testi semanticamente simili sono vicini tra loro. Ad esempio, le parole "cane" e "gatto" potrebbero avere embedding vicini perché sono entrambi animali domestici, mentre la parola "auto" potrebbe essere più lontana perché rappresenta un concetto diverso.

Gli embedding sono particolarmente utili nella ricerca semantica perché permettono di andare oltre la semplice corrispondenza di parole chiave. Ad esempio, se un utente cerca "veicolo", un motore di ricerca semantico può trovare documenti che parlano di "automobile" o "macchina", anche se questi termini non sono presenti nella query.

## Esempio

Consideriamo un semplice esempio per illustrare il concetto di embedding. Supponiamo di avere le seguenti frasi:

1. "Il cane abbaia nel parco."
2. "Il gatto miagola in casa."
3. "L'auto è parcheggiata nel garage."

Un modello di embedding potrebbe generare i seguenti vettori per queste frasi:

- "Il cane abbaia nel parco." → [0.2, 0.8, 0.1, 0.3]
- "Il gatto miagola in casa." → [0.3, 0.7, 0.2, 0.4]
- "L'auto è parcheggiata nel garage." → [0.8, 0.2, 0.9, 0.1]

In questo spazio vettoriale, le prime due frasi sono più vicine tra loro perché parlano di animali domestici, mentre la terza frase è più lontana perché parla di un veicolo.

## Pseudocodice

Ecco uno pseudocodice che illustra come potremmo generare e utilizzare gli embedding:

```rust
// Definizione di un embedding come vettore di float
struct Embedding {
    vettore: Vec<f32>,
}

// Funzione per generare un embedding da un testo
funzione genera_embedding(testo: &str) -> Embedding {
    // Utilizza un modello pre-addestrato per generare l'embedding
    let modello = carica_modello("modello.onnx");
    let tokens = tokenizza(testo);
    let input = prepara_input(tokens);
    let output = modello.esegui(input);
    
    Embedding {
        vettore: output.estrai_embedding(),
    }
}

// Funzione per calcolare la distanza tra due embedding
funzione distanza_embedding(a: &Embedding, b: &Embedding) -> f32 {
    // Calcola la distanza euclidea tra i due vettori
    let somma_quadrati = a.vettore.iter()
        .zip(b.vettore.iter())
        .map(|(x, y)| (x - y).powi(2))
        .sum::<f32>();
    
    somma_quadrati.sqrt()
}

// Funzione per trovare documenti simili
funzione trova_simili(query: &str, documenti: &[Documento]) -> Vec<Documento> {
    let query_embedding = genera_embedding(query);
    
    documenti.iter()
        .map(|doc| {
            let doc_embedding = genera_embedding(&doc.contenuto);
            let distanza = distanza_embedding(&query_embedding, &doc_embedding);
            (doc.clone(), distanza)
        })
        .sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap()) // Ordina per distanza crescente
        .map(|(doc, _)| doc)
        .collect()
}
```

## Risorse

Ecco alcune risorse utili per approfondire il concetto di embedding:

1. [Understanding Embeddings](https://jalammar.github.io/illustrated-word2vec/)
2. [Word Embeddings Explained](https://www.tensorflow.org/text/guide/word_embeddings)
3. [Sentence Embeddings with Transformers](https://www.sbert.net/)

## Esercizio

Per consolidare quanto appreso, prova a rispondere alle seguenti domande:

1. **Cos'è un embedding e come rappresenta il significato semantico del testo?**
2. **Qual è la differenza tra un embedding e una rappresentazione basata su keyword?**
3. **Come possiamo utilizzare gli embedding per trovare documenti simili?**

**Traccia di soluzione:**

1. Un embedding è un vettore di numeri che rappresenta il significato semantico di un testo. Ogni dimensione del vettore cattura un aspetto del significato, permettendo di confrontare testi in base alla loro similarità semantica.

2. Un embedding cattura il significato semantico del testo, permettendo di trovare documenti simili anche se non contengono le stesse parole chiave. Una rappresentazione basata su keyword, invece, si basa sulla corrispondenza esatta delle parole.

3. Possiamo utilizzare gli embedding per trovare documenti simili calcolando la distanza o la similarità tra l'embedding della query e gli embedding dei documenti. Documenti con embedding vicini sono semanticamente simili.
